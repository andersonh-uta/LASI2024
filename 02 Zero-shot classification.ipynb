{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47034b4a-4a61-4913-b208-647c4c802ea7",
   "metadata": {},
   "source": [
    "# Prerequisite: Install libraries\n",
    "You'll need the `bitsandbytes` library installed for the quantization step in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145b7608-b3eb-4144-ba64-f3b998aa1800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (0.43.1)\n",
      "Requirement already satisfied: torch in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from bitsandbytes) (2.3.1)\n",
      "Requirement already satisfied: numpy in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from torch->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from torch->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/andersonh/.cache/miniconda3/envs/LASI/lib/python3.11/site-packages (5.27.1)\n"
     ]
    }
   ],
   "source": [
    "# One more package we need for this notebook\n",
    "!pip3 install -U bitsandbytes\n",
    "\n",
    "# this one is maybe needed depending what model you use\n",
    "!pip3 install -U sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec969a98-7b6a-4009-8c58-5c043ab7ff2a",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification\n",
    "\n",
    "We can group supervised learning into a few different \"regimes\" based on the amount of training data:\n",
    "- Lots of training data: no special name for this.\n",
    "    - E.g.: a few thousand pictures of cats and dogs, labeled for what animal they portray.\n",
    "- Not a lot of data: few-shot learning.\n",
    "    - E.g.: a few dozen pictures of cats and dogs, labeled for what animal they portray.\n",
    "- Only one example: one-shot learning.\n",
    "    - E.g.: here is _one_ picture of a cat, and _one_ picture of a dog.\n",
    "- No training data: zero-shot learning.\n",
    "    - E.g.: I don't have any pictures, but here's a description of a cat and a description of a dog.\n",
    "\n",
    "Humans are great at one-shot and zero-shot learning.  Computers, historically have not been.  LLMs have done a lot to change that.\n",
    "\n",
    "In this notebook, we'll focus on _zero-shot learning_ using large, generative models.\n",
    "\n",
    "# Some major caveats and warnings\n",
    "\n",
    "Generative models--while very cool, and shockingly good at doing one-shot and zero-shot learning, _should not be used blindly._  There are some _major_ concerns around their use.  A non-exhaustive list:\n",
    "\n",
    "1. Data privacy.\n",
    "    1. if you're using a model that isn't hosted locally, be sure you know their data privacy policies.  Uploading data to ChatGPT could be a violation of IRB protocols or data privacy laws.\n",
    "    2. This is not an issue if you're hosting the the mode entirely locally.\n",
    "3. Reproducibility.\n",
    "    1. The same prompt won't always give the same results.  _This is the models working as they are supposed to.  It is impossible to fully prevent this behavior._  \n",
    "    2. With local models you can set a fixed random seed for reproducibility.  _But reproducibility and correctness are not the same thing._\n",
    "    3. Web-hosted models like ChatGPT might undergo significant changes with no notice.\n",
    "4. Hallucinations.\n",
    "    1. Generative models _are designed to hallucinate._  It is _mathematically impossible_ to prevent this without crippling the models.\n",
    "    2. If accuracy is critical, _do not use these results without verifying them._  (At which point, you might not be saving yourself any work).\n",
    "5. Output formats.\n",
    "    1. Text-to-text models don't reliably produce machine-readable outputs.  You may have to do a lot of manual work or post-processing to convert text outputs into a machine-readable format, e.g. to do a statistical analysis on them.\n",
    "6. Context windows.\n",
    "    1. LLMs can only \"remember\" so much text.  This is less of an issue now than it used to be, but for very long texts, you may have to find a way to work around this limitation.\n",
    "7. Instability.\n",
    "    1. Small changes to your prompts might result in different outputs for each document.\n",
    "    2. It is not possible to reason about how changes to the input will affect the output; these models are black boxes.\n",
    "8. Cost.\n",
    "    1. Generative LLMs aren't free.  They require _very_ high compute resources to run efficiently.  You're either buying GPUs, renting compute from somewhere like AWS, or paying a per-token price for API calls to something like ChatGPT.  This can get very expensive very quickly for some projects.\n",
    "8. Factual accuracy.\n",
    "   1. _LLMs do not have a notion of factual accuracy._  They only compute a _probable_ continuation of the text, _based on their training data_.  This can result in output that has the form of a factually true statement, _but the model is not aware of \"factual accuracy\" in either the inputs or outputs._\n",
    "9. Meta-linguistic references and math.\n",
    "    1. Generative models are consistently _awful_ at doing math, or at \n",
    "\n",
    "Generative models are not magic, they are not oracles, they are not universal problem solvers, they are not knowledge bases.  They are tools that generate text that looks like their training data.  Everything else they can do, while extremely impressive and very interesting, _is a happy accident._\n",
    "\n",
    "_However,_ they still have some very compelling and interesting use cases.  We'll explore one of them here: zero-shot classification.\n",
    "\n",
    "# Data\n",
    "\n",
    "We'll use the same data as for document classification, but with a few tweaks.  Just to make our job easier, we'll focus only on 1-star and 5-star reviews, and ask the model to tell the difference between them.  (We could ask it to just guess the number of stars directly, but it's faster for us to check a binary prediction, and it still illustrates the point for this workshop).  But, we'll change a few column names around, so we can track the original label (the \"ground truth\") and the model's outputs side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bac88e-a771-417a-990a-ed0367b7217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# use the smaller (10k reviews per star rating) test set--just for speed\n",
    "data = datasets.load_dataset(\"yelp_review_full\", split=\"test\")\n",
    "\n",
    "y = np.array(data[\"label\"])\n",
    "x = np.array(data[\"text\"])\n",
    "\n",
    "# only 1/5 star reviews.  Coded as 0-4 in the dataset.\n",
    "keep = (y == 0) | (y == 4)\n",
    "x = x[keep]\n",
    "y = y[keep]\n",
    "\n",
    "# convert to 0/1\n",
    "y = (y / 4).astype(int)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"Text\": x,\n",
    "    \"Ground Truth\": y,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f47d7-71fd-4e40-8f2f-d9b172b5adb7",
   "metadata": {},
   "source": [
    "# Pick and load a model\n",
    "\n",
    "Next, we'll need to pick and load a model.  We'll pick a model designed for chat-type texts (a la ChatGPT).  These tend to generate output that works well for zero-shot classification.  These also tend to be _very_ large models--many billions of parameters--so we'll probably need to employ some tricks to get it to actually run on our hardware.\n",
    "\n",
    "We'll use _quantization_, specifically.  This allows loading the model in _very_ low-precision numeric formats that should only have a minor impact on the final behavior.  If you're running this notebook somewhere with access to a big GPU--16gb of free VRAM or more--you can try out some of the other tricks that are commented out in the cell below.  You might get diffeent results, or at least faster runtimes.\n",
    "\n",
    "The code below uses the Huggingface-trained model, Zephyr-7b-beta.  This model doesn't give the best results from my testing, but it doesn't require accepting any terms and conditions, or requesting access; anyone can use it with no extra steps.  If you want to use the other models here, you need to make a Huggingface account, go to to the Model Hub, find the model in question, and follow the steps to access the gated model.  (it's not hard, but it's more steps than I want to deal with for this workshop).  I would recommend the \"instruct\" models from the options in the cell below, but try them all out and do some compare-and-contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9e0e52-d5d3-4ff2-9680-4462035bdff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ee75759754c85aca8e631001c2bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# recommended models--try both!\n",
    "# model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_name = \"HuggingFaceH4/mistral-7b-sft-beta\"\n",
    "\n",
    "# Mistral v0.3 tends to do better than HuggingFace's versions in my experience,\n",
    "# but these are gated models.  You need a Huggingface account to access them,\n",
    "# and need to accept their terms of use (basically just agreeing to the Apache\n",
    "# 2.0 license terms, which are absurdly permissive).\n",
    "# model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Mixtral models are \"mixture of experts\" models.  Very good results, but also\n",
    "# gated models, and the can be trickier to run.\n",
    "# model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "# model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Llama-3 is the current big hyped up model, but it is also gated and requires\n",
    "# a bit more to access.\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-instruct\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # trick 1: quantization.  Makes the model a lot smaller --> uses less VRAM and is easier to run.\n",
    "    # This will also handle device offloading, so some of the model will be executed on the GPU,\n",
    "    # some on the CPU.  This results in slowdowns as data is copied between devices.\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    ),\n",
    "    # set this explicitly to silence a warning\n",
    "    low_cpu_mem_usage=True,\n",
    "    \n",
    "    # trick 2: use flash attention to speed up the self-attention block.  Only works if you're\n",
    "    # on an NVidia Ampere or newer card.  You'll need to install the `flash-attn` library with pip,\n",
    "    # which can be a bit tricky to get working.\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "\n",
    "    # trick 3: use 16-bit floats, INSTEAD of quantization.\n",
    "    # `torch.float16` is a basic 16-bit float. Halves the memory usage of the models,\n",
    "    # but they still need a lot of memory to run.  Try this if you have an older GPU\n",
    "    # with a lot of VRAM.\n",
    "    # torch_dtype=torch.float16\n",
    ")\n",
    "# put the model in evaluation mode; this won't track gradients and should speed things\n",
    "# up a bit compared to being in training mode.\n",
    "model.eval()\n",
    "\n",
    "# only use .to(\"cuda\") if you can fit the whole model onto your GPU and are NOT using\n",
    "# quantization.  (or, feel free to try using it WITH quantization, but expect some\n",
    "# possible issues).\n",
    "# model = model.to(\"cuda\")\n",
    "\n",
    "# optional code to compile the model\n",
    "# import sys\n",
    "# if sys.version_info.minor <= 12 or torch.__version__.split(\".\")[1] >= \"4\":\n",
    "#     clf = torch.compile(clf)\n",
    "# else:\n",
    "#     print(\n",
    "#         f\"Cannot compile the model.  Need a Python version *prior to* 3.12 (you have: {sys.version}), or \"\n",
    "#         f\"a PyTorch version 2.4.0 or later (you have: {torch.__version__}\"\n",
    "#     )\n",
    "\n",
    "# if we don't specify the padding token ID, we'll get a bunch of warnings about it being\n",
    "# set automatically when we run inference.  This isn't a problem for the results, it's \n",
    "# just a bunch of extraneous output we don't want to see if we can avoid it.\n",
    "model.generation_config.pad_token_id = tok.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8d56d-33bc-4b8b-94c3-d4179a8f0b0d",
   "metadata": {},
   "source": [
    "# Prep work: prompts\n",
    "\n",
    "Generative models--especially instruction-tuned ones, and chat-oriented ones--typically need a prompt or carrier phrase to do zero-shot classification.  You can roughly think of these as being a set of instructions to give to the model, _but the model does not interpret these any different from normal input text._  Remember: the models are just predicting what word is likely to come next.  It just so happens that for chat-oriented models, they tend to be trained on data where one user asks another to do something, and the other user does it.  But you _cannot_ guarantee that the model will always follow directions, because it has no real notion of \"directions.\"  Just text that it needs to keep running autocomplete on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a28f1b-f64e-4038-813e-a75bf35b2430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED_IDS:\n",
      "tensor([[  523, 28766,  1838, 28766, 28767,    13, 28737,   622,  3084,   368,\n",
      "           395,   272,  2245,   302,   264,  4058, 28723, 28705,  5919,   875,\n",
      "          1575,   272,  4058,   390,  2477,  5278,   442,  7087, 28723, 28705,\n",
      "           415,  2245,   302,   272,  4058,   349,   390,  6104, 28747,   851,\n",
      "          4708, 28725,   272,  7345,  5614,   477,   272, 26361,  1605,   538,\n",
      "          7503,  4028,  8599, 28725, 14191,   264, 14358, 20622,   477,   652,\n",
      "          4123,  3238, 28723, 28705,   420,   538,   460,   272, 12351,  8133,\n",
      "           302, 10686, 12139,   304,  3944, 28725, 10421, 28733,   406,   528,\n",
      "           391,  2131, 12950,  2547, 28723, 28705,   560,   652,  1633,   349,\n",
      "          1545,   633, 28747, 21435,  2468, 28725,   562, 28429, 28725, 12950,\n",
      "          2547, 28725,   395,   264,  2286,   680,   302,   396,  1183, 27621,\n",
      "           304,  1019,  2939,  2323,   602, 28723, 28705,  2941, 28733,  8896,\n",
      "          8504,   302,   272,  4028, 28742, 28713,  6402,   771,   993,   459,\n",
      "           347, 12846, 28725,   562,   456,   633,  5007,  8288,  1188,  9081,\n",
      "         28725,  1019,   513,   456,   907,  9055,   297,   272,   633,  3238,\n",
      "           349,   264,  3468,  8654,  1401,   272, 11742, 28723,     2, 28705,\n",
      "            13, 28789, 28766,   489, 11143, 28766, 28767,    13,  1014,  4058,\n",
      "          4498,   594,   272, 14358, 20622,   302,   272,   633,  4708,   477,\n",
      "          8599, 28742, 28713,  3454,   771,   304, 12308,   369,   272,   633,\n",
      "          9184,   460, 28429,   304,   506,   396,  1183, 27621,   304,  2939,\n",
      "          2323,   602, 28723,   415,  4058,   835, 12308,   369,   456,   633,\n",
      "          5007,  8288,  9081, 28723, 17158,   356,   272, 12280, 28725,   378,\n",
      "          3969,   680,  5278,   821,  7087, 28723,     2]], device='cuda:0')\n",
      "\n",
      "DECODED TEXT:\n",
      "<|user|>\n",
      "I will provide you with the text of a review.  Please classify the review as either positive or negative.  The text of the review is as follows: This album, the latest release from the legendary drone metal band Earth, marks a radical departure from their normal style.  Gone are the giant walls of guitar feedback and slow, drawn-out meandering riffs.  In their place is something new: repetitive, but structured, riffs, with a bit more of an acoustic and even country twang.  Die-hard fans of the band's older work may not be pleased, but this new direction holds much promise, even if this first offering in the new style is a pretty rough around the edges.</s> \n",
      "<|assistant|>\n",
      "The review mentions the radical departure of the new album from Earth's previous work and suggests that the new songs are structured and have an acoustic and country twang. The review also suggests that this new direction holds promise. Based on the passage, it seems more positive than negative.</s>\n"
     ]
    }
   ],
   "source": [
    "# our \"carrier phrase\" which is written in the form of instructions for the model to carry out.\n",
    "prompt = (\n",
    "    \"I will provide you with the text of a review.  Please classify the review \"\n",
    "    \"as either positive or negative.  The text of the review is as follows: {}\"\n",
    ")\n",
    "\n",
    "# A sample review--crafted to have an ambiguous sentiment--that we'll use for some demos.\n",
    "review = (\n",
    "    \"This album, the latest release from the legendary drone metal band Earth, marks a radical \"\n",
    "    \"departure from their normal style.  Gone are the giant walls of guitar feedback and slow, \"\n",
    "    \"drawn-out meandering riffs.  In their place is something new: repetitive, but structured, \"\n",
    "    \"riffs, with a bit more of an acoustic and even country twang.  Die-hard fans of the band's\"\n",
    "    \" older work may not be pleased, but this new direction holds much promise, even if this \"\n",
    "    \"first offering in the new style is a pretty rough around the edges.\"\n",
    ")\n",
    "\n",
    "# Now create the messages in a \"chat template\".  The template is different for each model,\n",
    "# but the tokenizer can convert things for it.  We just need to set the data up first.\n",
    "# Note: different models will have different roles available.  \"user\" and \"assistant\"\n",
    "# are common.  Some models have a \"system\" role.  We could pass several messages, alternating\n",
    "# between user and assistant, to generate a \"backlog\" of messages to further bias the model\n",
    "# towards a particular kind of output, but we'll just provide the user message here.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt.format(review)} ,\n",
    "]\n",
    "# format the messages into the expected chat format\n",
    "model_inputs = tok.apply_chat_template(\n",
    "    messages,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# run the prepared texts through the model and get the outputs.\n",
    "generated_ids = model.generate(\n",
    "    # input data\n",
    "    **{k:v.to(\"cuda\") for k,v in model_inputs.items()},\n",
    "    # stop asking for more tokens after 256 tokens\n",
    "    max_new_tokens=256,\n",
    "    # sample randomly from high-likelihood next tokens\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# the outputs are currently just a tensor of numeric IDs, each corresponding to a\n",
    "# token in the model's vocabulary.\n",
    "print(\"GENERATED_IDS:\")\n",
    "print(generated_ids)\n",
    "\n",
    "# The tokenizer can decode these numeric IDs into human-readable strings.\n",
    "print(\"\\nDECODED TEXT:\")\n",
    "print(tok.batch_decode(generated_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180a853-89b9-4898-aab1-7c3ce70ed68b",
   "metadata": {},
   "source": [
    "# Post-processing the output\n",
    "\n",
    "Notice how we have both our input and the model's response in a single string.  We need to do a little work to parse this out.  All models have some way to indicate when the \"speaker\" changes; Zephyr uses `<|system|>`, `<|user|>`, and `<|assistant|>` to mark the start of the system, user, and assistant texts, respectively, and uses `</s>` to mark the end of an entire dialog.  We can use this to parse out the different dialog \"turns\" and just show the model's response.\n",
    "\n",
    "We'll also wrap the above logic up into a function to make life a bit easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b71f8140-a868-4730-9f29-d335b32443f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textwrap import wrap\n",
    "\n",
    "def annotate(carrier_phrase, text, model=model, tok=tok):\n",
    "    \"\"\"Run the text `text` through the model `model`, and capture just the model's output.\"\"\"\n",
    "    # format the carrier phrase with the `text` input and apply the chat template.\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": carrier_phrase.format(text)}\n",
    "    ]\n",
    "    model_inputs = tok.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "    # get the model's responses\n",
    "    generated_ids = model.generate(\n",
    "        **{k:v.to(\"cuda\") for k,v in model_inputs.items()},\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    output = tok.batch_decode(generated_ids)[0]\n",
    "    \n",
    "    # split the text into system, user, and assistant chunks.\n",
    "    if model_name in (\"HuggingFaceH4/zephyr-7b-beta\", \"HuggingFaceH4/mistral-7b-sft-beta\"):\n",
    "        # sanity check: sometimes the tokens we want don't appear.\n",
    "        if \"<|user|>\" not in output or \"<|assistant|>\" not in output:\n",
    "            return \"\", \"\"\n",
    "        output = re.split(r\"(<\\|user\\|>|</s>|<\\|system\\|>|<\\|assistant\\|>)\", output)\n",
    "        user = output.index(\"<|user|>\")\n",
    "        assistant = output.index(\"<|assistant|>\")\n",
    "    elif model_name in (\"mistralai/Mistral-7B-v0.3\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"mistralai/Mixtral-8x7B-v0.1\"):\n",
    "        if \"[INST]\" not in output or \"[/INST]\" not in output:\n",
    "            return \"\", \"\"\n",
    "        output = re.split(r\"(\\[INST\\]|\\[/INST\\]|</s>)\", output)\n",
    "        user = output.index(\"[INST]\")\n",
    "        assistant = output.index(\"[/INST]\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Oops, you need to add logic for the outputs from {model_name}.\"\n",
    "            \"You'll need to add logic to identify the start/end tokens for user \"\n",
    "            \"and assistant roles.\"\n",
    "        )\n",
    "\n",
    "    user_input = wrap(output[user + 1].strip(), subsequent_indent=\"\\t\")\n",
    "    model_response = wrap(output[assistant + 1].strip(), subsequent_indent=\"\\t\")\n",
    "    return \"\\n\".join(user_input), \"\\n\".join(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890be945-a804-4a33-a767-0f29eefcb352",
   "metadata": {},
   "source": [
    "Let's run our sample review through this new logic a few times, and see how different the results are across a few different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62e3d543-e10b-4a9f-aefa-d86e3995b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation number 1\n",
      "\tMODEL SAYS: Negative.\n",
      "\n",
      "Annotation number 2\n",
      "\tMODEL SAYS: Negative.\n",
      "\n",
      "Annotation number 3\n",
      "\tMODEL SAYS: This review is positive.\n",
      "\n",
      "Annotation number 4\n",
      "\tMODEL SAYS: Positive\n",
      "\n",
      "Annotation number 5\n",
      "\tMODEL SAYS: Negative.\n",
      "\n",
      "Annotation number 6\n",
      "\tMODEL SAYS: Negative.\n",
      "\n",
      "Annotation number 7\n",
      "\tMODEL SAYS: This review is negative.\n",
      "\n",
      "Annotation number 8\n",
      "\tMODEL SAYS: The review is positive.\n",
      "\n",
      "Annotation number 9\n",
      "\tMODEL SAYS: The reviewer has classified the album as a \"radical departure\" from\n",
      "\tthe traditional style of the band, using unconventional musical\n",
      "\telements and styles that are structured and acoustic. While die-hard\n",
      "\tfans of earlier efforts may be displeased, the reviewer sees\n",
      "\tpotential in the new direction. However, the album is only considered\n",
      "\t\"rough around the edges\" by the reviewer, indicating that it may not\n",
      "\tbe fully successful. Therefore, this review is negative.\n",
      "\n",
      "Annotation number 10\n",
      "\tMODEL SAYS: The review is positive, as the reviewer expresses excitement and\n",
      "\tanticipation for Earth's new direction, while still recognizing some\n",
      "\tshortcomings of the first iteration of the new style.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    _, label = annotate(prompt, review, model=model, tok=tok)\n",
    "    print(f\"Annotation number {i+1}\")\n",
    "    print(f\"\\tMODEL SAYS: {label.strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76dcf7-7bb1-44a4-8ef0-08f265a7358e",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "The model's output is pretty verbose.  This might be fine if we're manually reading them and putting values into a spreadsheet, but it would be nice if we could get machine-readable output directly from the model.  We can try to bias the model's outputs by \"engineering\" the prompt to do what we want--hence, \"prompt engineering.\"\n",
    "\n",
    "Some notes about prompt engineering:\n",
    "- A good starting point: just add extra instructions to the prompt to tell the model what you want it to do.  Despite the models not actually \"following directions,\" this can work surprisingly well.\n",
    "- This is a _very_ fiddly process.  There is no \"best\" prompt (and no real way to evaluate the quality of prompts), and most of the tweaking ends up being educated guesswork.  It'll also be different for each model.  You can build a good intiution for what to do with your prompts, but there's still a lot of guess-and-check.\n",
    "- _You may not be able to make the model do what you want._  It isn't always possible to force its output into a desired form.\n",
    "- You might end up making the outputs worse.  There's no way to know until you try.\n",
    "\n",
    "We'll do the simple thing: we'll tell the model to just say 0 or 1, and not explain its reasoning.  We'll see how that goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac92c605-6418-40b5-bd71-603f5e893e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation number 1 for the Earth review\n",
      "\tMODEL SAYS: The review is negative as it says that fans of the band's old work may\n",
      "\tnot be pleased, and the first offering in the new style is rough\n",
      "\taround the edges. Therefore, the response should be '0'.\n",
      "\n",
      "Annotation number 2 for the Earth review\n",
      "\tMODEL SAYS: 1\n",
      "\n",
      "Annotation number 3 for the Earth review\n",
      "\tMODEL SAYS: Positive.\n",
      "\n",
      "Annotation number 4 for the Earth review\n",
      "\tMODEL SAYS: Please provide the full text of the review. Additionally, based on the\n",
      "\tinformation you have thus far, it is unclear which style the listener\n",
      "\tis more familiar with or which release might have been the listener's\n",
      "\tfavorite - both of which can impact how the album is received. Can\n",
      "\tyou please provide additional context or information about the\n",
      "\tlistener's past experiences with Earth's music?\n",
      "\n",
      "Annotation number 5 for the Earth review\n",
      "\tMODEL SAYS: The statement is negative towards the reviewed material. Therefore,\n",
      "\tonly '0' should be responded.\n",
      "\n",
      "Annotation number 6 for the Earth review\n",
      "\tMODEL SAYS: Positive (1).\n",
      "\n",
      "Annotation number 7 for the Earth review\n",
      "\tMODEL SAYS: The review is positive and only responds with the word \"1.\"\n",
      "\n",
      "Annotation number 8 for the Earth review\n",
      "\tMODEL SAYS: The review can be classified as negative (0).\n",
      "\n",
      "Annotation number 9 for the Earth review\n",
      "\tMODEL SAYS: This is negative feedback.\n",
      "\n",
      "Annotation number 10 for the Earth review\n",
      "\tMODEL SAYS: Negative. Please provide me with the word '0'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"I will provide you with the text of a review.  Please classify the review \"\n",
    "    \"as either positive or negative.  If the review is positive, you should only \"\n",
    "    \"respond with the word '1'.  If the review is negative, you should only \"\n",
    "    \"respond with the word '0'.  The text of the review is as follows: {}\"\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Annotation number {i+1} for the Earth review\")\n",
    "    _, label = annotate(prompt, review, model=model, tok=tok)\n",
    "    print(f\"\\tMODEL SAYS: {label.strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b60a0-d1a5-47ff-a98f-c9e66357b8b4",
   "metadata": {},
   "source": [
    "Well, that worked a bit better--not perfect, but better than it was before.  Now let's run this over our actual data and compare the model to the ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e99c65-fae5-4195-87ff-86f5e78d137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "TEXT: I got 'new' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\nI took the tire over to Flynn's and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he'd give me a new tire \\\"this time\\\". \\nI will never go back to Flynn's b/c of the way this guy treated me and the simple fact that they gave me a used tire!\n",
      "\n",
      "MODEL SAYS: This is a negative review.\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: Don't waste your time.  We had two different people come to our house to give us estimates for a deck (one of them the OWNER).  Both times, we never heard from them.  Not a call, not the estimate, nothing.\n",
      "\n",
      "MODEL SAYS: Negative. Please respond with the word '0'.\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: All I can say is the worst! We were the only 2 people in the place for lunch, the place was freezing and loaded with kids toys! 2 bicycles, a scooter, and an electronic keyboard graced the dining room. A fish tank with filthy, slimy fingerprints smeared all over it is there for your enjoyment.\\n\\nOur food came... no water to drink, no tea, medium temperature food. Of course its cold, just like the room, I never took my jacket off! The plates are too small, you food spills over onto some semi-clean tables as you sit in your completely worn out booth seat. The fried noodles were out of a box and nasty, the shrimp was mushy, the fried rice was bright yellow.\\n\\nWe asked for water, they brought us 1 in a SOLO cup for 2 people. I asked for hot tea, they said 10 minutes. What Chinese restaurant does not have hot tea available upon request?\\n\\nOver all.... my first and last visit to this place. The only good point was that it was cheap, and deservingly so.\n",
      "\n",
      "MODEL SAYS: The review is negative.\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: I have been to this restaurant twice and was disappointed both times. I won't go back. The first time we were there almost 3 hours. It took forever to order and then forever for our food to come and the place was empty. When I complained the manager was very rude and tried to blame us for taking to long to order. It made no sense, how could we order when the waitress wasn't coming to the table? After arguing with me he ended up taking $6 off of our $200+ bill. Ridiculous. If it were up to me I would have never returned. Unfortunately my family decided to go here again tonight. Again it took a long time to get our food. My food was cold and bland, my kids food was cold. My husbands salmon was burnt to a crisp and my sister in law took one bite of her trout and refused to eat any more because she claims it was so disgusting. The wedding soup and bread were good, but that's it! My drink sat empty throughout my meal and never got refilled even when I asked. Bad food, slow service and rude managers. I'll pass on this place if my family decides to go again. Not worth it at all with all the other good Italian options around.\n",
      "\n",
      "MODEL SAYS: Negative\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: Food was NOT GOOD at all! My husband & I ate here a couple weeks ago for the first time. I ordered a salad & basil pesto cream pasta & my husband ordered the spinach & feta pasta. The salad was just a huge plate of spring mix (nothing else in it) with WAY to much vinegar dressing. My lettuce was drowning in the vinegar. My pesto pasta had no flavor (did not taste like a cream sauce to me) & the pesto was so runny/watery & way too much sauce not enough noodles. My husband's pasta had even less flavor than mine. We ate about a quarter of the food & couldn't even finish it. We took it home & it was so bad I didn't even eat my leftovers. And I hate wasting food!! Plus the prices are expensive for the amount of food you get & of course the poor quality. Don't waste your time eating here. There are much better Italian restaurants in Pittsburgh.\n",
      "\n",
      "MODEL SAYS: Negative\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: To keep it short and sweet: Save yourself $100. Buy a good board game, your alcohol of choice, order a pizza, and invite your friends over. \\n\\nWhat an incredible disappointment. After seeing the enticing commercials so many times, we decided to give this place a try on a double date. I understand the prices of the play cards and won't dispute them; however, the food was incredibly over-priced, came out COLD (as in, sat on a counter without warmers for a minimum of 30 minutes) and I literally had to ask the bartender if there was any vodka in my drink. It was pure juice. $38 for three shots that had little-no alcohol in them. (Not to mention, my glass was dirty, and I saw the bartender scoop the glass into the ice basin because she was too lazy to use the sanitary scoop. I know the Food and Beverage Commission would be as disappointed as I was.) The service was terrible. Don't ask for anything from your waiter, as they are a little too busy on their cell phones or conversing amongst themselves. \\n\\nWas it fun to be in an adult-themed arcade? Yes. If you're looking for a good atmosphere to go with friends to play games, I suppose I would advise you give it a shot. I would never recommend their food, customer service, or drinks. Save yourself the money and stay home, or go for a traditional bowling, figure skating, roller-blading, rock climbing, basically any other physically-entertaining themed date instead.\n",
      "\n",
      "MODEL SAYS: In this case, the text is negative, so the response should be \"0\".\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: The words \\\"epic fail\\\" get thrown around a lot these days....but I really feel like they apply in this situation.\\n\\nWe went on Friday, April 2 and arrived at 5:10pm.  It was busy, but not crowded....no waiting for a table.  Half price apps and drinks -- we ordered at 5:20.\\n\\nThe food (just appetizers, mind you...) didn't arrive till almost 6pm.  Drinks were ordered and were unbelievably slow.  We placed one order for 6 draft beers at 6:15...they arrived at 6:52.  *37 minutes for beer.*\\n\\nBy 7:00, we were canceling food orders which we'd given up on after waiting almost an hour.  We just wanted to leave.\\n\\nInstead of comping anything, they added 18% gratuity to each of our bills.  Uhhhh yeah.  Thanks for giving me one more reason never to come back.   We finally were able to leave around 7:30.\\n\\nBar Louie:  You are dead to me.\n",
      "\n",
      "MODEL SAYS: The review is negative. Please respond with '0'.\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: I was looking to get out of the apartment on a really nice, sunny day and we decided to drive to Waterfront and walk around. All around great day until we hit Bar Louie for some drinks and appetizers.\\n\\nI am giving it one star, though it deserves none, because our waitress was nice, if a bit inattentive, and the hummus app we ordered was pretty damn good. Outside of that Bar Louie leaves a lot to be desired.\\n\\nThis is probably the first place I've been to where they charge $10 and up for all mixed drinks. To me, that is beyond ridiculous. Bar Louie is not an upscale restaurant, as much as they wish they were, and paying almost $60 for three drinks and two appetizers is insanity defined. \\n\\nAs I mentioned the waitress was friendly, but she definitely did not come back and check on us enough. I really wanted to like this place because of things I had heard and the drinks we had were really damn good, but if it want to spend that kind of money, I'll go somewhere that I can get good service.\n",
      "\n",
      "MODEL SAYS: POSITIVE  However, the review mentions the lack of service and the\n",
      "\texorbitant prices of mixed drinks which may be viewed as a negative\n",
      "\taspect for some.\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: I hate this place.\\nIt's very loud, the service is very poor, and the food is so-so.\\nIf you want good Chinese in Pittsburgh, try China Palace (Shadyside) or Sesame Inn (Station Square or North Hills).  They're quieter, with very good food & service.\n",
      "\n",
      "MODEL SAYS: Your response should be: 0. The review is negative and you should only\n",
      "\trespond with the word '0'.\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: This is by far my favorite Panera location in the Pittsburgh area. Friendly, plenty of room to sit, and good quality food & coffee. Panera is a great place to hang out and read the news - they even have free WiFi! Try their toasted sandwiches, especially the chicken bacon dijon.\n",
      "\n",
      "MODEL SAYS: For this review, the response provided is 1 as the review is positive.\n",
      "\n",
      "GROUND TRUTH: 1\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# now, on our actual review data\n",
    "for _, row in data.iloc[:10].iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    ground_truth = row[\"Ground Truth\"]\n",
    "    _, label = annotate(prompt, text, model=model, tok=tok)\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"TEXT: {text}\")\n",
    "    print(f\"\\nMODEL SAYS: {label}\")\n",
    "    print(f\"\\nGROUND TRUTH: {ground_truth}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912b200-015d-42bf-b9af-55eadc7d69dc",
   "metadata": {},
   "source": [
    "Great!  Now we can have the model run over all of our texts and label them, with no need to do any labeling ourselves.  _Crucial caveat:_ even though our prompt includes instructions to only reply with a number, 0/1/2, _we cannot guarantee that the model's outputs will always be what we requested._  So you may run into some issues if you blindly assume that the output will always be an integer.\n",
    "\n",
    "_Crucial caveat:_ these models are _non-deterministic_.  The same inputs might yield different outputs.  We can see that by running the same text through the model a few different times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d70e8470-fa54-4332-a9e1-f0de85744b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421fa32e50a04d04abade36e10c71266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# create the column for storing the \"raw\" response from the model.\n",
    "# If it already exists--e.g. if you're re-running this call--then don't.\n",
    "# This avoids overwriting pervious runs' outputs.\n",
    "if \"Zero-shot Label\" not in data:\n",
    "    data[\"Zero-shot Label\"] = \"\"\n",
    "\n",
    "# only the first 1000--otherwise this will take many, many hours to run.\n",
    "# This will still take a while, though.\n",
    "for i in tqdm(data.index[:1000]):\n",
    "    # skip any texts already annotated--in case we re-run this code a few times\n",
    "    if data.loc[i, \"Zero-shot Label\"] != \"\":\n",
    "        continue\n",
    "    _, label = annotate(prompt, data.loc[i, \"Text\"], model=model, tok=tok)\n",
    "    data.loc[i, \"Zero-shot Label\"] = label\n",
    "\n",
    "# try converting everything to an int and see if it causes any problems--\n",
    "# this will result in some missing values in most cases!\n",
    "def try_convert(maybe_int):\n",
    "    try:\n",
    "        return int(maybe_int.strip())\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "data[\"Converted Zero-shot Label\"] = data[\"Zero-shot Label\"].map(try_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67110ac-5664-4b99-a4e3-c0c37991c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 29/1000 outputs to numeric values\n",
      "F1: 0.675776397515528\n",
      "Ground Truth\n",
      "1    17\n",
      "0    12\n",
      "Name: count, dtype: int64\n",
      "Converted Zero-shot Label\n",
      "1.0    18\n",
      "0.0    11\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "preds = data[data[\"Zero-shot Label\"] != \"\"]\n",
    "\n",
    "print(f\"Successfully converted {data['Converted Zero-shot Label'].count()}/{preds.shape[0]} outputs to numeric values\")\n",
    "\n",
    "# check some error metrics\n",
    "from sklearn.metrics import f1_score\n",
    "preds = preds.dropna()\n",
    "f1 = f1_score(preds[\"Ground Truth\"], preds[\"Converted Zero-shot Label\"], average=\"macro\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "print(preds[\"Ground Truth\"].value_counts())\n",
    "print(preds[\"Converted Zero-shot Label\"].value_counts())\n",
    "fix, ax = plt.subplots()\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=preds[\"Ground Truth\"].astype(int),\n",
    "    y_pred=preds[\"Converted Zero-shot Label\"].astype(int),\n",
    "    xticks_rotation=\"vertical\",\n",
    "    ax=ax,\n",
    "    colorbar=False,\n",
    "    # labels=[\"Negative\", \"Positive\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4e354-525c-4600-81e0-98e4a3c73244",
   "metadata": {},
   "source": [
    "# Some concluding notes\n",
    "\n",
    "The above answers will be a bit worse than they could be for a few reasons:\n",
    "- The models we used have 7 billion parameters.  This is on the small side for most generative models.  Larger models might give more consistent results, but be _extremely_ difficult to actually run.  And extremely slow.\n",
    "- Quantization can effect text generation a bit more than other tasks.  Text generation is a lot more sensitive to initial conditions.  (this is likely not a _huge_ effect\n",
    "- We didn't do a whole lot with prompt engineering.  E.g., we didn't provide a \"system\" message, which might have provided better results.\n",
    "- We didn't tweak any of the generation options, like the `temperature` or `top_k` values (we left these at the defaults).\n",
    "- Other models than Zephyr might give better results.  Mistral v0.3-instruct and Llama-3 might be good candidates to try.\n",
    "\n",
    "We could do a few things with these results.  We could use them to jump-start a classifier model, or use them as suggestions for a human coder working through a lot of data (spot-checking can be easier than labelling from scratch!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bab20-2d69-4624-b2f7-27ccb45ca36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
