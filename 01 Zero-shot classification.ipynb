{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9d8fb7-8ef9-45f0-a188-8be975551449",
   "metadata": {},
   "source": [
    "# Prerequisite: Install libraries\n",
    "If you're running locally and have an AMD GPU, sorry, you're gonna have to figure out ROCm installation on your own.\n",
    "\n",
    "If you're running locally, have an NVidia GPU, and use conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ace55e-bff7-4957-865b-9a7505c8f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries needed for this notebook--run this cell if running the notebook locally.\n",
    "# Restart your Jupyter session afterwards, because of the ipywidgets installation.\n",
    "# !conda install --yes numpy scikit-learn matplotlib tqdm cudatoolkit\n",
    "# !conda install --yes pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# !pip3 install -U transformers accelerate datasets ipywidgets bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9cdfe-4042-483f-a078-e3b793acdfea",
   "metadata": {},
   "source": [
    "If you're running this in Google Colab, a lot of libraries are pre-installed in the Colab runtime.  Run the following cell to install the remaining libraries, and then re-start your session just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9a4824-2ca4-45b6-a42e-4f67eff7784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U accelerate datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d89a4-a15e-432b-bf12-b1d6bb9bcbb0",
   "metadata": {},
   "source": [
    "You may need to install `sentencepiece` and `protobuf` depending what models you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174afaeb-8f53-45bf-adc5-0c5c00558b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is maybe needed depending what model you use\n",
    "# !pip3 install -U sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec969a98-7b6a-4009-8c58-5c043ab7ff2a",
   "metadata": {},
   "source": [
    "# Preface: some libraries to know about\n",
    "\n",
    "[Huggingface](https://huggingface.co/)'s [various Python libraries](https://huggingface.co//docs) are probably the best place to get started with LLMs, and what we'll be using today.  There's also [Langchain](https://www.langchain.com/), which is quickly getting more and more popular, but we won't be covering it today.\n",
    "\n",
    "Some of the big libraries in the Huggingface ecosystem you should know about (click each library name to go to its documentation):\n",
    "- [`transformers`](https://huggingface.co/docs/transformers/index): the library we'll be using.  General-purpose Transformer models for working with text.  (Almost all modern LLMs are based on the Transformer architecture).\n",
    "- [`diffusers`](https://huggingface.co/docs/diffusers/index): like `transformers`, but for diffusion models, which output images and videos.\n",
    "- [`accelerate`](https://huggingface.co/docs/accelerate/index): provides various optimizations for running models in Huggingface's other libraries.\n",
    "- [`optimum`](https://huggingface.co/docs/optimum/index): provides various hardware-specific optimizations for Huggingface models.\n",
    "- [`gradio`](https://www.gradio.app/docs): dashboard library, focused on deploying machine learning applications (not just LLMs).\n",
    "- [`datasets`](https://huggingface.co/docs/datasets/index): lots and lots of datasets to play with!\n",
    "- [`peft`](https://huggingface.co/docs/peft/index): provides various \"parameter-efficient fine-tuning\" methods that let you fine-tune big models with much lower RAM requirements.\n",
    "\n",
    "We'll be using just `transformers`, `accelerate`, and `datasets`.  We'll also use the PyTorch models in `transformers`, though there are also Tensorflow and JAX models, and some `transformers`-specific training tools.  Feel free to explore those on your own time.\n",
    "\n",
    "Compared to `scikit-learn` (from the last example), training PyTorch models takes more code.  None of it is particularly complicated, or particularly difficult.  The hard parts will be all the stuff around the code, rather than the code itself.\n",
    "\n",
    "# Zero-shot classification with generative models\n",
    "\n",
    "## Some major caveats and warnings\n",
    "\n",
    "Generative models--while very cool, and shockingly good at doing one-shot and zero-shot learning, _should not be used blindly._  There are some _major_ concerns around their use.  A non-exhaustive list:\n",
    "\n",
    "1. Data privacy.\n",
    "    1. if you're using a model that isn't hosted locally, be sure you know their data privacy policies.  Uploading data to ChatGPT could be a violation of IRB protocols or data privacy laws.\n",
    "    2. This is not an issue if you're hosting the the mode entirely locally.\n",
    "3. Reproducibility.\n",
    "    1. The same prompt won't always give the same results.  _This is the models working as they are supposed to.  It is impossible to fully prevent this behavior._  \n",
    "    2. With local models you can set a fixed random seed for reproducibility.  _But reproducibility and correctness are not the same thing._\n",
    "    3. Web-hosted models like ChatGPT might undergo significant changes with no notice.\n",
    "4. Hallucinations.\n",
    "    1. Generative models _are designed to hallucinate._  It is _mathematically impossible_ to prevent this without crippling the models.\n",
    "    2. If accuracy is critical, _do not use these results without verifying them._  (At which point, you might not be saving yourself any work).\n",
    "5. Output formats.\n",
    "    1. Text-to-text models don't reliably produce machine-readable outputs.  You may have to do a lot of manual work or post-processing to convert text outputs into a machine-readable format, e.g. to do a statistical analysis on them.\n",
    "6. Context windows.\n",
    "    1. LLMs can only \"remember\" so much text.  This is less of an issue now than it used to be, but for very long texts, you may have to find a way to work around this limitation.\n",
    "7. Instability.\n",
    "    1. Small changes to your prompts might result in different outputs for each document.\n",
    "    2. It is not possible to reason about how changes to the input will affect the output; these models are black boxes.\n",
    "8. Cost.\n",
    "    1. Generative LLMs aren't free.  They require _very_ high compute resources to run efficiently.  You're either buying GPUs, renting compute from somewhere like AWS, or paying a per-token price for API calls to something like ChatGPT.  This can get very expensive very quickly for some projects.\n",
    "8. Factual accuracy.\n",
    "   1. _LLMs do not have a notion of factual accuracy._  They only compute a _probable_ continuation of the text, _based on their training data_.  This can result in output that has the form of a factually true statement, _but the model is not aware of \"factual accuracy\" in either the inputs or outputs._\n",
    "9. Meta-linguistic references and math.\n",
    "    1. Generative models are consistently _awful_ at doing math, or at \n",
    "\n",
    "Generative models are not magic, they are not oracles, they are not universal problem solvers, they are not knowledge bases.  They are tools that generate text that looks like their training data.  Everything else they can do, while extremely impressive and very interesting, _is a happy accident._\n",
    "\n",
    "_However,_ they still have some very compelling and interesting use cases.  We'll explore one of them here: zero-shot classification.\n",
    "\n",
    "# Zero-Shot Classification\n",
    "\n",
    "\"Zero-shot classification\" = making predictions by describing the task, or describing the different categories, without providing labeled data.  Zero-shot classification is _hard_, but LLMs are surprisingly good at it--with some important caveats that we'll see in a bit.  Generative LLMs in particular show a lot of promise for this kind of application, which is where we'l focus for this notebook.\n",
    "\n",
    "# Data and prediction task\n",
    "\n",
    "Just to keep it simple, we'll do a classic sentiment analysis task.  We'll pull a dataset of Yelp reviews, keep only the 1 and 5 star reviews, and ask a model to predict if this is a positive (5-star) or negative (1-star) review.\n",
    "\n",
    "We're intentionally keeping it simple for this workshop; you can use the same general steps we show here with far more complicated tasks, but the general design of the code won't change much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bac88e-a771-417a-990a-ed0367b7217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# use the smaller (10k reviews per star rating) test set--just for speed\n",
    "data = datasets.load_dataset(\"yelp_review_full\", split=\"test\")\n",
    "\n",
    "y = np.array(data[\"label\"])\n",
    "x = np.array(data[\"text\"])\n",
    "\n",
    "# only 1/5 star reviews.  Coded as 0-4 in the dataset.\n",
    "keep = (y == 0) | (y == 4)\n",
    "x = x[keep]\n",
    "y = y[keep]\n",
    "\n",
    "# convert to 0/1\n",
    "y = (y / 4).astype(int)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"Text\": x,\n",
    "    \"Ground Truth\": y,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f47d7-71fd-4e40-8f2f-d9b172b5adb7",
   "metadata": {},
   "source": [
    "# Pick and load a model\n",
    "\n",
    "Next, we'll need to pick and load a model.  We'll pick a model designed for chat-type texts.  These tend to generate output that works well for zero-shot classification.\n",
    "\n",
    "These are _big_ models, though, so we need to do some tricks to be able to run them.  We'll use _quantization_.  This allows loading the model in _very_ low-precision numeric formats that should only have a minor impact on the final behavior.  (Other tricks might work better depending on your specific hardware, but this allows us to run the models with very low VRAM usage).\n",
    "\n",
    "The code below uses the Huggingface-trained model, Zephyr-7b-beta.  This model doesn't give the best results from my testing, but the other models require accepting the models' licenses.  You also need a Huggingface account, and you need to set up your Huggingface credentials in your environment to access them, after accepting their terms.  After doing that, though, switching models is as simple as changing what lines below are and aren't commented out; the rest of the code doesn't change at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9e0e52-d5d3-4ff2-9680-4462035bdff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6078683378464eb78b6600720867afc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# recommended models--try both!\n",
    "# model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_name = \"HuggingFaceH4/mistral-7b-sft-beta\"\n",
    "\n",
    "# Mistral v0.3 tends to do better than HuggingFace's versions in my experience,\n",
    "# but these are gated models.  You need a Huggingface account to access them,\n",
    "# and need to accept their terms of use (basically just agreeing to the Apache\n",
    "# 2.0 license terms, which are absurdly permissive).\n",
    "# model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Mixtral models are \"mixture of experts\" models.  Very good results, but also\n",
    "# gated models, and the can be trickier to run.\n",
    "# model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "# model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Llama-3 is the current big hyped up model, but it is also gated and requires\n",
    "# a bit more to access.\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-instruct\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # trick 1: quantization.  Makes the model a lot smaller --> uses less VRAM and is easier to run.\n",
    "    # This will also handle device offloading, so some of the model will be executed on the GPU,\n",
    "    # some on the CPU.  This results in slowdowns as data is copied between devices.\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    ),\n",
    "    # set this explicitly to silence a warning\n",
    "    low_cpu_mem_usage=True,\n",
    "    \n",
    "    # trick 2: use flash attention to speed up the self-attention block.  Only works if you're\n",
    "    # on an NVidia Ampere or newer card.  You'll need to install the `flash-attn` library with pip,\n",
    "    # which can be a bit tricky to get working.\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "\n",
    "    # trick 3: use 16-bit floats, INSTEAD of quantization.\n",
    "    # `torch.float16` is a basic 16-bit float. Halves the memory usage of the models,\n",
    "    # but they still need a lot of memory to run.  Try this if you have an older GPU\n",
    "    # with a lot of VRAM.\n",
    "    # torch_dtype=torch.float16\n",
    ")\n",
    "# put the model in evaluation mode; this won't track gradients and should speed things\n",
    "# up a bit compared to being in training mode.\n",
    "model.eval()\n",
    "\n",
    "# only use .to(\"cuda\") if you can fit the whole model onto your GPU and are NOT using\n",
    "# quantization.  (or, feel free to try using it WITH quantization, but expect some\n",
    "# possible issues).\n",
    "# model = model.to(\"cuda\")\n",
    "\n",
    "# optional code to compile the model\n",
    "# import sys\n",
    "# if sys.version_info.minor <= 12 or torch.__version__.split(\".\")[1] >= \"4\":\n",
    "#     clf = torch.compile(clf)\n",
    "# else:\n",
    "#     print(\n",
    "#         f\"Cannot compile the model.  Need a Python version *prior to* 3.12 (you have: {sys.version}), or \"\n",
    "#         f\"a PyTorch version 2.4.0 or later (you have: {torch.__version__}\"\n",
    "#     )\n",
    "\n",
    "# if we don't specify the padding token ID, we'll get a bunch of warnings about it being\n",
    "# set automatically when we run inference.  This isn't a problem for the results, it's \n",
    "# just a bunch of extraneous output we don't want to see if we can avoid it.\n",
    "model.generation_config.pad_token_id = tok.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8d56d-33bc-4b8b-94c3-d4179a8f0b0d",
   "metadata": {},
   "source": [
    "# Prep work: prompts\n",
    "\n",
    "Instruction-tuned models, and chat models generally, need some sort of prompt to guide the text they generate.  We'll handle this in a pretty simple way: we'll just use a \"carrier phrase\" for each review, which will provide instructions on what we want the model to do.  For every review, we'll embed the review text into this carrier phrase and send it to the model.\n",
    "\n",
    "_Note:_ the model is not \"following instructions,\" per se.  The model is continuing to generate text that looks like it would come after the text the user provides.  For most good instruction-tuned models, this ends up generating text that looks like it's following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a28f1b-f64e-4038-813e-a75bf35b2430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\AppData\\Local\\miniconda3\\envs\\LASI\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:674: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED_IDS:\n",
      "tensor([[  523, 28766,  1838, 28766, 28767,    13, 12069,  9051,   272, 21790,\n",
      "           302,   272,  2296,  4058, 28747,   464, 21934, 28745,  1667, 13281,\n",
      "           288,   560,   415,   560,   642,  2340, 10011, 28742,   349,   272,\n",
      "         10036,  4708,   477,   272, 26361,  1605,   538,  7503,  4028, 28725,\n",
      "          8599, 28725,   304,   264, 14358, 20622,   477,   652,  3454,  3238,\n",
      "         28725,   690,   349,  3917,   298, 13265,  1287,  1202, 28733,  8896,\n",
      "          8504, 28723, 28705,   420,   538,   460,   272,  8133,   302, 10686,\n",
      "          1255, 25382, 28725,  8939,   395,   264,   680,  2939, 28733,   262,\n",
      "         17262,  4697,  2622,   395,  6315,  4057, 28725,  3944, 28725,   304,\n",
      "         21435,  2468, 12950,  2547, 28723, 28705,   415,  4708,   349,   708,\n",
      "          2108,  5917,   821,   652,  3454, 21446, 28725,   304,   349,   737,\n",
      "           298,   347,   264, 24720,  4905,   778,   272,  4028, 28742, 28713,\n",
      "         18255, 28723,     2, 28705,    13, 28789, 28766,   489, 11143, 28766,\n",
      "         28767,    13, 21436,  1197,     2]], device='cuda:0')\n",
      "\n",
      "DECODED TEXT:\n",
      "<|user|>\n",
      "Please identify the sentiment of the following review: 'Hex; Or Printing In The Infernal Method' is the fourth album from the legendary drone metal band, Earth, and a radical departure from their previous style, which is likely to upset many die-hard fans.  Gone are the walls of guitar distortion, replaced with a more country-influenced sound with clearly defined, slow, and repetitive riffs.  The album is no less heavy than their previous releases, and is like to be a controversial entry into the band's catalog.</s> \n",
      "<|assistant|>\n",
      "Negative</s>\n"
     ]
    }
   ],
   "source": [
    "# our \"carrier phrase\" which is written in the form of instructions for the model to carry out.\n",
    "prompt = (\n",
    "    \"Please identify the sentiment of the following review: {}\"\n",
    ")\n",
    "\n",
    "# A sample review--crafted to have an ambiguous sentiment--that we'll use for some demos.\n",
    "review = (\n",
    "    \"'Hex; Or Printing In The Infernal Method' is the fourth album from the legendary \"\n",
    "    \"drone metal band, Earth, and a radical departure from their previous style, which \"\n",
    "    \"is likely to upset many die-hard fans.  Gone are the walls of guitar distortion, \"\n",
    "    \"replaced with a more country-influenced sound with clearly defined, slow, and \"\n",
    "    \"repetitive riffs.  The album is no less heavy than their previous releases, and is \"\n",
    "    \"like to be a controversial entry into the band's catalog.\"\n",
    ")\n",
    "\n",
    "# Now create the messages in a \"chat template\".  The template is different for each model,\n",
    "# but the tokenizer can convert things for it.  We just need to set the data up first.\n",
    "# Note: different models will have different roles available.  \"user\" and \"assistant\"\n",
    "# are common.  Some models have a \"system\" role.  We could pass several messages, alternating\n",
    "# between user and assistant, to generate a \"backlog\" of messages to further bias the model\n",
    "# towards a particular kind of output, but we'll just provide the user message here.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt.format(review)} ,\n",
    "]\n",
    "# format the messages into the expected chat format\n",
    "model_inputs = tok.apply_chat_template(\n",
    "    messages,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# run the prepared texts through the model and get the outputs.\n",
    "generated_ids = model.generate(\n",
    "    # input data\n",
    "    **{k:v.to(\"cuda\") for k,v in model_inputs.items()},\n",
    "    # stop asking for more tokens after 256 tokens\n",
    "    max_new_tokens=256,\n",
    "    # sample randomly from high-likelihood next tokens\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# the outputs are currently just a tensor of numeric IDs, each corresponding to a\n",
    "# token in the model's vocabulary.\n",
    "print(\"GENERATED_IDS:\")\n",
    "print(generated_ids)\n",
    "\n",
    "# The tokenizer can decode these numeric IDs into human-readable strings.\n",
    "print(\"\\nDECODED TEXT:\")\n",
    "print(tok.batch_decode(generated_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180a853-89b9-4898-aab1-7c3ce70ed68b",
   "metadata": {},
   "source": [
    "# Post-processing the output\n",
    "\n",
    "Notice how we have both our input and the model's response in a single string.  We need to do a little work to parse this out.  All models have some way to indicate when the \"speaker\" changes; Zephyr uses `<|system|>`, `<|user|>`, and `<|assistant|>` to mark the start of the system, user, and assistant texts, respectively, and uses `</s>` to mark the end of an entire dialog.  We can use this to parse out the different dialog \"turns\" and just show the model's response.\n",
    "\n",
    "_Note_: in the Huggingface library, the tokenizers have a special attribute--`added_tokens_decoder`--which you can user to programmatically get all special tokens that the model/chat template might add.  We won't use that for this code, and will instead hard-code the particular tokens we're interested in.\n",
    "\n",
    "We'll also wrap the above logic up into a function to make life a bit easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71f8140-a868-4730-9f29-d335b32443f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textwrap import wrap\n",
    "\n",
    "def annotate(carrier_phrase, text, model=model, tok=tok):\n",
    "    \"\"\"Run the text `text` through the model `model`, and capture just the model's output.\"\"\"\n",
    "    # format the carrier phrase with the `text` input and apply the chat template.\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": carrier_phrase.format(text)}\n",
    "    ]\n",
    "    model_inputs = tok.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    # get the model's responses\n",
    "    generated_ids = model.generate(\n",
    "        **{k:v.to(\"cuda\") for k,v in model_inputs.items()},\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    output = tok.batch_decode(generated_ids)[0]\n",
    "    \n",
    "    # split the text into system, user, and assistant chunks.\n",
    "    if model_name in (\"HuggingFaceH4/zephyr-7b-beta\", \"HuggingFaceH4/mistral-7b-sft-beta\"):\n",
    "        # sanity check: sometimes the tokens we want don't appear.\n",
    "        if \"<|user|>\" not in output or \"<|assistant|>\" not in output:\n",
    "            return \"\", \"\"\n",
    "        output = re.split(r\"(<\\|user\\|>|</s>|<\\|system\\|>|<\\|assistant\\|>)\", output)\n",
    "        user = output.index(\"<|user|>\")\n",
    "        assistant = output.index(\"<|assistant|>\")\n",
    "    elif model_name in (\"mistralai/Mistral-7B-v0.3\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"mistralai/Mixtral-8x7B-v0.1\"):\n",
    "        if \"[INST]\" not in output or \"[/INST]\" not in output:\n",
    "            return \"\", \"\"\n",
    "        output = re.split(r\"(\\[INST\\]|\\[/INST\\]|</s>)\", output)\n",
    "        user = output.index(\"[INST]\")\n",
    "        assistant = output.index(\"[/INST]\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Oops, you need to add logic for the outputs from {model_name}.\"\n",
    "            \"You'll need to add logic to identify the start/end tokens for user \"\n",
    "            \"and assistant roles.\"\n",
    "        )\n",
    "\n",
    "    user_input = wrap(output[user + 1].strip(), subsequent_indent=\"\\t\")\n",
    "    model_response = wrap(output[assistant + 1].strip(), subsequent_indent=\"\\t\")\n",
    "    return \"\\n\".join(user_input), \"\\n\".join(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890be945-a804-4a33-a767-0f29eefcb352",
   "metadata": {},
   "source": [
    "# Caution: Non-deterministic outputs\n",
    "\n",
    "Let's run our sample review through this new logic a few times, and see how different the results are across a few different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e3d543-e10b-4a9f-aefa-d86e3995b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation number 1\n",
      "\tMODEL SAYS: The sentiment of the review is likely positive or neutral, as the\n",
      "\treviewer indicates that the album is no less heavy than Earth's\n",
      "\tprevious releases and is likely to appeal to fans of other musical\n",
      "\tgenres beyond drone metal. However, the reviewer notes that the\n",
      "\tradical departure from the band's previous style may upset some die-\n",
      "\thard fans of the drone metal genre. Therefore, the reviewer's\n",
      "\tsentiment may fall somewhere between enjoyment of the album as a\n",
      "\tcreative exploration and understanding of the possible frustrations\n",
      "\tsome longtime fans may experience.\n",
      "\n",
      "Annotation number 2\n",
      "\tMODEL SAYS: The sentiment of the review is positive towards the new style of\n",
      "\tEarth's album.\n",
      "\n",
      "Annotation number 3\n",
      "\tMODEL SAYS: The sentiment of the following review is neutral.\n",
      "\n",
      "Annotation number 4\n",
      "\tMODEL SAYS: The sentiment of the review is neutral, as it provides a balanced\n",
      "\tassessment without expressing any explicit positive or negative\n",
      "\topinions.\n",
      "\n",
      "Annotation number 5\n",
      "\tMODEL SAYS: The sentiment of the review is negative.\n",
      "\n",
      "Annotation number 6\n",
      "\tMODEL SAYS: The sentiment of the review is positive. The author suggests that\n",
      "\t'Hex; or Printing in the Infernal Method' is a radical departure from\n",
      "\tEarth's previous style, but the new country-influenced sound is a\n",
      "\tsuccess. The author praises the album for being heavy, slow, and\n",
      "\trepetitive, and suggests that it is highly likely to be a\n",
      "\tcontroversial entry into the band's catalog.\n",
      "\n",
      "Annotation number 7\n",
      "\tMODEL SAYS: This review is negative.\n",
      "\n",
      "Annotation number 8\n",
      "\tMODEL SAYS: The sentiment is uncertain as it is presented as a neutral analysis of\n",
      "\tthe album's artistic change, without expressing any explicit\n",
      "\tsentiments about it.\n",
      "\n",
      "Annotation number 9\n",
      "\tMODEL SAYS: The sentiment of this review is mixed.\n",
      "\n",
      "Annotation number 10\n",
      "\tMODEL SAYS: The sentiment of the review is slightly negative, with the reviewer\n",
      "\texpressing doubts about the new direction taken by the band and\n",
      "\tpotential disappointment from fans.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    _, label = annotate(prompt, review, model=model, tok=tok)\n",
    "    print(f\"Annotation number {i+1}\")\n",
    "    print(f\"\\tMODEL SAYS: {label.strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76dcf7-7bb1-44a4-8ef0-08f265a7358e",
   "metadata": {},
   "source": [
    "This is going to be less of a problem with some models than others, but it _is not possible_ to guarantee that the same model will have the same outputs for a given input, when run multiple times.\n",
    "\n",
    "# Prompt Engineering\n",
    "\n",
    "The model's output is pretty verbose.  This might be fine if we're manually reading them and putting values into a spreadsheet, but it would be nice if we could get machine-readable output directly from the model.  We can try to bias the model's outputs by \"engineering\" the prompt to do what we want--hence, \"prompt engineering.\"\n",
    "\n",
    "Some notes about prompt engineering:\n",
    "- A good starting point: just add extra instructions to the prompt to tell the model what you want it to do.  Despite the models not actually \"following directions,\" this can work surprisingly well.\n",
    "- Sometimes you need to repeat the same instructions a few times to really hammer the point home.\n",
    "- This is a _very_ fiddly process.  There is no \"best\" prompt (and no real way to evaluate the quality of prompts), and most of the tweaking ends up being educated guesswork.  It'll also be different for each model.  You can build a good intiution for what to do with your prompts, but there's still a lot of guess-and-check.\n",
    "- _You may not be able to make the model do what you want._  It isn't always possible to force its output into a desired form.\n",
    "- You might end up making the outputs worse.  There's no way to know until you try.\n",
    "\n",
    "We'll do the simple thing: we'll tell the model to just say 0 or 1, and not explain its reasoning.  We'll see how that goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac92c605-6418-40b5-bd71-603f5e893e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation number 1 for the Earth review\n",
      "\tMODEL SAYS: 0\n",
      "\n",
      "Annotation number 2 for the Earth review\n",
      "\tMODEL SAYS: The sentiment is primarily negative, so the number is 0.\n",
      "\n",
      "Annotation number 3 for the Earth review\n",
      "\tMODEL SAYS: 0\n",
      "\n",
      "Annotation number 4 for the Earth review\n",
      "\tMODEL SAYS: The sentiment is positive.  To be specific, the text does not focus on\n",
      "\tthe positive or negative elements of the album, and instead is\n",
      "\tdescriptive. It describes the change in the band's style and the\n",
      "\tlikely response from diehard fans, while also acknowledging that the\n",
      "\talbum is no less heavy than their previous releases. Therefore, it\n",
      "\tdoes not give a positive or negative sentiment about the album, and\n",
      "\tshould be given a neutral ranking. The final text may be described as\n",
      "\tneutral.\n",
      "\n",
      "Annotation number 5 for the Earth review\n",
      "\tMODEL SAYS: 0\n",
      "\n",
      "Annotation number 6 for the Earth review\n",
      "\tMODEL SAYS: 0\n",
      "\n",
      "Annotation number 7 for the Earth review\n",
      "\tMODEL SAYS: 0 (Negative)\n",
      "\n",
      "Annotation number 8 for the Earth review\n",
      "\tMODEL SAYS: Negative: 0\n",
      "\n",
      "Annotation number 9 for the Earth review\n",
      "\tMODEL SAYS: 1\n",
      "\n",
      "Annotation number 10 for the Earth review\n",
      "\tMODEL SAYS: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Please identify whether the following text is primarily positive or negative \"\n",
    "    \"in sentiment.  If it is primarily positive, say '1'.  if it is primarily negative, \"\n",
    "    \"say '0'.  Do not explain your reasoning, and only provide the number corresponding \"\n",
    "    \"to the sentiment.  The text is: {}\"\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Annotation number {i+1} for the Earth review\")\n",
    "    _, label = annotate(prompt, review, model=model, tok=tok)\n",
    "    print(f\"\\tMODEL SAYS: {label.strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b60a0-d1a5-47ff-a98f-c9e66357b8b4",
   "metadata": {},
   "source": [
    "Well, that worked a bit better--not perfect, but better than it was before.  Now let's run this over our actual data and compare the model to the ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e99c65-fae5-4195-87ff-86f5e78d137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "TEXT: I got 'new' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\nI took the tire over to Flynn's and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he'd give me a new tire \\\"this time\\\". \\nI will never go back to Flynn's b/c of the way this guy treated me and the simple fact that they gave me a used tire!\n",
      "\n",
      "MODEL SAYS: Please enter a number (1 or 0) to identify the primary sentiment of\n",
      "\tthe text:  1 or 0:\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: Don't waste your time.  We had two different people come to our house to give us estimates for a deck (one of them the OWNER).  Both times, we never heard from them.  Not a call, not the estimate, nothing.\n",
      "\n",
      "MODEL SAYS: 1\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: All I can say is the worst! We were the only 2 people in the place for lunch, the place was freezing and loaded with kids toys! 2 bicycles, a scooter, and an electronic keyboard graced the dining room. A fish tank with filthy, slimy fingerprints smeared all over it is there for your enjoyment.\\n\\nOur food came... no water to drink, no tea, medium temperature food. Of course its cold, just like the room, I never took my jacket off! The plates are too small, you food spills over onto some semi-clean tables as you sit in your completely worn out booth seat. The fried noodles were out of a box and nasty, the shrimp was mushy, the fried rice was bright yellow.\\n\\nWe asked for water, they brought us 1 in a SOLO cup for 2 people. I asked for hot tea, they said 10 minutes. What Chinese restaurant does not have hot tea available upon request?\\n\\nOver all.... my first and last visit to this place. The only good point was that it was cheap, and deservingly so.\n",
      "\n",
      "MODEL SAYS: 0\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: I have been to this restaurant twice and was disappointed both times. I won't go back. The first time we were there almost 3 hours. It took forever to order and then forever for our food to come and the place was empty. When I complained the manager was very rude and tried to blame us for taking to long to order. It made no sense, how could we order when the waitress wasn't coming to the table? After arguing with me he ended up taking $6 off of our $200+ bill. Ridiculous. If it were up to me I would have never returned. Unfortunately my family decided to go here again tonight. Again it took a long time to get our food. My food was cold and bland, my kids food was cold. My husbands salmon was burnt to a crisp and my sister in law took one bite of her trout and refused to eat any more because she claims it was so disgusting. The wedding soup and bread were good, but that's it! My drink sat empty throughout my meal and never got refilled even when I asked. Bad food, slow service and rude managers. I'll pass on this place if my family decides to go again. Not worth it at all with all the other good Italian options around.\n",
      "\n",
      "MODEL SAYS: 0.\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: Food was NOT GOOD at all! My husband & I ate here a couple weeks ago for the first time. I ordered a salad & basil pesto cream pasta & my husband ordered the spinach & feta pasta. The salad was just a huge plate of spring mix (nothing else in it) with WAY to much vinegar dressing. My lettuce was drowning in the vinegar. My pesto pasta had no flavor (did not taste like a cream sauce to me) & the pesto was so runny/watery & way too much sauce not enough noodles. My husband's pasta had even less flavor than mine. We ate about a quarter of the food & couldn't even finish it. We took it home & it was so bad I didn't even eat my leftovers. And I hate wasting food!! Plus the prices are expensive for the amount of food you get & of course the poor quality. Don't waste your time eating here. There are much better Italian restaurants in Pittsburgh.\n",
      "\n",
      "MODEL SAYS: 0\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: To keep it short and sweet: Save yourself $100. Buy a good board game, your alcohol of choice, order a pizza, and invite your friends over. \\n\\nWhat an incredible disappointment. After seeing the enticing commercials so many times, we decided to give this place a try on a double date. I understand the prices of the play cards and won't dispute them; however, the food was incredibly over-priced, came out COLD (as in, sat on a counter without warmers for a minimum of 30 minutes) and I literally had to ask the bartender if there was any vodka in my drink. It was pure juice. $38 for three shots that had little-no alcohol in them. (Not to mention, my glass was dirty, and I saw the bartender scoop the glass into the ice basin because she was too lazy to use the sanitary scoop. I know the Food and Beverage Commission would be as disappointed as I was.) The service was terrible. Don't ask for anything from your waiter, as they are a little too busy on their cell phones or conversing amongst themselves. \\n\\nWas it fun to be in an adult-themed arcade? Yes. If you're looking for a good atmosphere to go with friends to play games, I suppose I would advise you give it a shot. I would never recommend their food, customer service, or drinks. Save yourself the money and stay home, or go for a traditional bowling, figure skating, roller-blading, rock climbing, basically any other physically-entertaining themed date instead.\n",
      "\n",
      "MODEL SAYS: 0\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: The words \\\"epic fail\\\" get thrown around a lot these days....but I really feel like they apply in this situation.\\n\\nWe went on Friday, April 2 and arrived at 5:10pm.  It was busy, but not crowded....no waiting for a table.  Half price apps and drinks -- we ordered at 5:20.\\n\\nThe food (just appetizers, mind you...) didn't arrive till almost 6pm.  Drinks were ordered and were unbelievably slow.  We placed one order for 6 draft beers at 6:15...they arrived at 6:52.  *37 minutes for beer.*\\n\\nBy 7:00, we were canceling food orders which we'd given up on after waiting almost an hour.  We just wanted to leave.\\n\\nInstead of comping anything, they added 18% gratuity to each of our bills.  Uhhhh yeah.  Thanks for giving me one more reason never to come back.   We finally were able to leave around 7:30.\\n\\nBar Louie:  You are dead to me.\n",
      "\n",
      "MODEL SAYS: 0\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: I was looking to get out of the apartment on a really nice, sunny day and we decided to drive to Waterfront and walk around. All around great day until we hit Bar Louie for some drinks and appetizers.\\n\\nI am giving it one star, though it deserves none, because our waitress was nice, if a bit inattentive, and the hummus app we ordered was pretty damn good. Outside of that Bar Louie leaves a lot to be desired.\\n\\nThis is probably the first place I've been to where they charge $10 and up for all mixed drinks. To me, that is beyond ridiculous. Bar Louie is not an upscale restaurant, as much as they wish they were, and paying almost $60 for three drinks and two appetizers is insanity defined. \\n\\nAs I mentioned the waitress was friendly, but she definitely did not come back and check on us enough. I really wanted to like this place because of things I had heard and the drinks we had were really damn good, but if it want to spend that kind of money, I'll go somewhere that I can get good service.\n",
      "\n",
      "MODEL SAYS: 0\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: I hate this place.\\nIt's very loud, the service is very poor, and the food is so-so.\\nIf you want good Chinese in Pittsburgh, try China Palace (Shadyside) or Sesame Inn (Station Square or North Hills).  They're quieter, with very good food & service.\n",
      "\n",
      "MODEL SAYS: 0\n",
      "\n",
      "GROUND TRUTH: 0\n",
      "--------------------\n",
      "--------------------\n",
      "TEXT: This is by far my favorite Panera location in the Pittsburgh area. Friendly, plenty of room to sit, and good quality food & coffee. Panera is a great place to hang out and read the news - they even have free WiFi! Try their toasted sandwiches, especially the chicken bacon dijon.\n",
      "\n",
      "MODEL SAYS: 1. The text expresses a positive sentiment towards Panera's location,\n",
      "\tincluding its quality food and friendly atmosphere. Therefore, the\n",
      "\tsentiment is primarily positive.\n",
      "\n",
      "GROUND TRUTH: 1\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# now, on our actual review data\n",
    "for _, row in data.iloc[:10].iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    ground_truth = row[\"Ground Truth\"]\n",
    "    _, label = annotate(prompt, text, model=model, tok=tok)\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"TEXT: {text}\")\n",
    "    print(f\"\\nMODEL SAYS: {label}\")\n",
    "    print(f\"\\nGROUND TRUTH: {ground_truth}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912b200-015d-42bf-b9af-55eadc7d69dc",
   "metadata": {},
   "source": [
    "Great!  Now we can have the model run over all of our texts and label them, with no need to do any labeling ourselves.  _Crucial caveat:_ even though our prompt includes instructions to only reply with a number, 0/1/2, _we cannot guarantee that the model's outputs will always be what we requested._  So you may run into some issues if you blindly assume that the output will always be an integer.\n",
    "\n",
    "_Crucial caveat:_ these models are _non-deterministic_.  The same inputs might yield different outputs.  We can see that by running the same text through the model a few different times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d70e8470-fa54-4332-a9e1-f0de85744b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdc3f565af74b5ebe2256deab52b0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# create the column for storing the \"raw\" response from the model.\n",
    "# If it already exists--e.g. if you're re-running this call--then don't.\n",
    "# This avoids overwriting pervious runs' outputs.\n",
    "if \"Zero-shot Label\" not in data:\n",
    "    data[\"Zero-shot Label\"] = \"\"\n",
    "\n",
    "# only the first 100 documents--otherwise this will take many, many hours to run.\n",
    "# This will still take a while, though.\n",
    "for i in tqdm(data.index[:100]):\n",
    "    # skip any texts already annotated--in case we re-run this code a few times\n",
    "    if data.loc[i, \"Zero-shot Label\"] != \"\":\n",
    "        continue\n",
    "    _, label = annotate(prompt, data.loc[i, \"Text\"], model=model, tok=tok)\n",
    "    data.loc[i, \"Zero-shot Label\"] = label\n",
    "\n",
    "# try converting everything to an int and see if it causes any problems--\n",
    "# this will result in some missing values in most cases!\n",
    "def try_convert(maybe_int):\n",
    "    try:\n",
    "        return int(maybe_int.strip())\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "data[\"Converted Zero-shot Label\"] = data[\"Zero-shot Label\"].map(try_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67110ac-5664-4b99-a4e3-c0c37991c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 50/100 outputs to numeric values\n",
      "F1: 0.8599439775910365\n",
      "Ground Truth\n",
      "0    26\n",
      "1    24\n",
      "Name: count, dtype: int64\n",
      "Converted Zero-shot Label\n",
      "1.0    27\n",
      "0.0    23\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "preds = data[data[\"Zero-shot Label\"] != \"\"]\n",
    "\n",
    "print(f\"Successfully converted {data['Converted Zero-shot Label'].count()}/{preds.shape[0]} outputs to numeric values\")\n",
    "\n",
    "# check some error metrics\n",
    "from sklearn.metrics import f1_score\n",
    "preds = preds.dropna()\n",
    "f1 = f1_score(preds[\"Ground Truth\"], preds[\"Converted Zero-shot Label\"], average=\"macro\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "print(preds[\"Ground Truth\"].value_counts())\n",
    "print(preds[\"Converted Zero-shot Label\"].value_counts())\n",
    "fix, ax = plt.subplots()\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=preds[\"Ground Truth\"].astype(int),\n",
    "    y_pred=preds[\"Converted Zero-shot Label\"].astype(int),\n",
    "    xticks_rotation=\"vertical\",\n",
    "    ax=ax,\n",
    "    colorbar=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4e354-525c-4600-81e0-98e4a3c73244",
   "metadata": {},
   "source": [
    "# Some concluding notes\n",
    "\n",
    "The above answers will be a bit worse than they could be for a few reasons:\n",
    "- The models we used have 7 billion parameters.  This is on the small side for most generative models.  Larger models might give more consistent results, but be _extremely_ difficult to actually run.  And extremely slow.\n",
    "- Quantization can effect text generation a bit more than other tasks.  Text generation is a lot more sensitive to initial conditions.  (this is likely not a _huge_ effect\n",
    "- We didn't do a whole lot with prompt engineering.  E.g., we didn't provide a \"system\" message, which might have provided better results.\n",
    "- We didn't tweak any of the generation options, like the `temperature` or `top_k` values (we left these at the defaults).\n",
    "- Other models than Zephyr might give better results.  Mistral v0.3-instruct and Llama-3 might be good candidates to try.\n",
    "\n",
    "We could do a few things with these results.  We could use them to jump-start a classifier model, or use them as suggestions for a human coder working through a lot of data (spot-checking can be easier than labelling from scratch!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bab20-2d69-4624-b2f7-27ccb45ca36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
