{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b658646-e132-486a-acce-b041c266933f",
   "metadata": {},
   "source": [
    "# Document Annotation\n",
    "\n",
    "Almost all tasks you want to do can be decsribed as document annotation tasks: you have some text, and you want to apply a label or set of labels to it.  Maybe they're class labels (document classification), or a continuing value (regression).  LLMs are great for this--and you don't need the state-of-the-art models to get extremely good results, making this angle much more accessible than the large generative models tend to be.\n",
    "\n",
    "We'll re-use the same classification task from the zero-shot learning examples: identify positive/negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cde9c4d-c3e2-4e3e-92cb-30311fd46241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# use the smaller (10k reviews per star rating) test set--just for speed\n",
    "data = datasets.load_dataset(\"yelp_review_full\", split=\"test\")\n",
    "\n",
    "y = np.array(data[\"label\"])\n",
    "x = np.array(data[\"text\"])\n",
    "\n",
    "# SPEED TWEAK: only 1/5 star reviews.  Coded as 0-4 in the dataset.\n",
    "# Comment these 4 lines out to use all 5 star ratings.\n",
    "keep = (y == 0) | (y == 4)\n",
    "x = x[keep]\n",
    "y = y[keep]\n",
    "y = (y / 4).astype(int)\n",
    "\n",
    "# train-test split; use 80% for training\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, train_size=0.8, stratify=y)\n",
    "\n",
    "print(f\"{len(train_x):,} training observations, {len(test_x):,} testing observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3a0a6-0e17-4179-835e-42424b916cfb",
   "metadata": {},
   "source": [
    "# Digression: LLMs may not be necessary\n",
    "\n",
    "Document annotation--especially classification--is an extremely well-trod area of study.  There are a lot of extremely fast ways to do document classification with very high accuracy, and you should always try these out before reaching for LLMs.\n",
    "\n",
    "As a quick demo, we'll use an extremely simple, no-frills modeling approach: Bag-of-Words + Naive Bayes.\n",
    "- Bag-of-Words: convert text into a numeric/vectorized representation by counting how often the document uses each possible word.\n",
    "- Naive Bayes: for each feature (word), calculate the conditional probability of each class (star rating), given the value of the current feature (number of times the word was used).  To make predictions, calculate these probabilities for each document and feature, then assign it to the class with the highest probability.  (further details are beyond the scope of this workshop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6aefd-2671-4f1c-9be4-370f3fad61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = Pipeline([\n",
    "    # filter out super rate words and super common words--words should be in >10 documents\n",
    "    # and in <= 50% of all documents.  Extremely low and high frequency words rarely tell\n",
    "    # us much.\n",
    "    (\"vectorizer\", CountVectorizer(min_df=10, max_df=0.5)),\n",
    "    (\"clf\", ComplementNB()),\n",
    "])\n",
    "\n",
    "# `%time` is a Jupyter \"magic command\" that will time how long the following statement takes,\n",
    "# and print out the duration.\n",
    "%time clf.fit(train_x, train_y)\n",
    "%time preds = clf.predict(test_x)\n",
    "    \n",
    "# Calculate F1, a pretty common classification error metric\n",
    "from sklearn.metrics import r2_score, f1_score\n",
    "f1 = f1_score(test_y, preds, average=\"macro\")\n",
    "print(f\"Macro F1 on testing data (1 = perfect classification): {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482957b-166b-4b5d-acf1-d5f7e953f6c7",
   "metadata": {},
   "source": [
    "And for good measure, let's generate a confusion matrix to visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48e681-e972-40b5-9c3c-c3a9b47b9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "# star labels are 0-4 in the dataset; convert to 1-5 for displaying here.\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=test_y + 1,\n",
    "    y_pred=preds + 1,\n",
    "    ax=ax,\n",
    "    colorbar=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716d219-41ee-43cc-81a5-0414d6947b1d",
   "metadata": {},
   "source": [
    "So, in under a second, running on a single CPU core, we can get _really_ good predictions, using older and simpler methods.  LLMs, as we'll see, require a lot more compute power (i.e. dedicated GPUs), and are a lot slower.  But they can be more accuracy.\n",
    "\n",
    "There are very few document annotation tasks where you _need_ an LLM.  They often provide the best-in-class accuracy (but not always by a huge margin), but at the cost of added time and compute power, which might be a deal breaker.\n",
    "\n",
    "# Document Classification with LLMs\n",
    "\n",
    "## Heads up: Some tweaks for speed\n",
    "\n",
    "LLMs and neural networks can take a _very_ long time to train.  We're going to make a few tweaks throughout this notebook to make them run faster, often by simplifying what they're actually doing.  We'll make two _big_ speed tweaks:\n",
    "1. We're doing 1-versus-5-star classification, rathe than 1/2/3/4/5-star classification.  This leaves us with less data to process, and is a simpler problem to solve, so the model should be able to converge more quickly.\n",
    "2. We're freezing the parameters of the LLM itself.  Normally, we'd be updating the weights of LLM as we train, in addition to the single densely connected layer that gets appended to the end of it to do classification.  For this notebook, we'll only be training that last layer.  This lets us iterate through the data more quickly, but might require more iterations to converge on a good solution.  Note that, by doing this, we're just using the LLM as a (really good, but slow) text vectorizer.  This is a perfectly sensible thing to do, especially in more compute-constrained settings where fine-tuning the whole model might not be feasible.  But, in those cases, you'd probably feed all your texts through the LLM to get the vectorized outputs, then save those, and train a model of your choice on them directly.  This would be a bit more complicated, but could be a lot more convenient (and faster, if you need to make more than one pass over your data).\n",
    "3. We're limiting ourselves to one pass over the data.  Normally we would keep iterating for as long as it takes for the model to converge.\n",
    "\n",
    "If we didn't do these tweaks, all we'd get is a longer runtime; nothing is really getting hidden or made simpler.  If you want to run this code without these tweaks, look for comments that start with \"SPEED TWEAK:\" throughout the next few cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226d437-fc40-4769-b16e-1ce07716194c",
   "metadata": {},
   "source": [
    "# Picking a Model\n",
    "\n",
    "There are _lots_ of LLM models out there for classification and regression tasks--more than there are in the generative world.  And you can run a lot more of them on consumer-grade hardware, so there's also more to pick from in practical terms.  However, most of the different models are largely interchangeable for most tasks.\n",
    "\n",
    "There are a few models you can use to get started in the `transformers` library:\n",
    "- `bert-base-uncased` and `bert-base-cased`: the classic, standard BERT models that started the current wave of Transformer models.  Still a great choice for almost eveything!  (`bert-base-uncased` is _not_ case-sensitive; it treats \"English\" and \"ENGLISH\" as the same; `bert-base-cased` _is_ case-sensitive).\n",
    "- `distilbert/distilbert-base-uncased` and `distilbert/distilbert-bsae-cased`: DistilBERT, a smaller/reduced-size version of the classic BERT models.  It's about twice as fast, uses about half the memory, and is about 95% as accurate.  (A _great_ default first model, due to its relative speed).\n",
    "- `FacebookAI/roberta-base`: RoBERTa, a variant of BERT trained with a slightly different training objective, which typically out-performs BERT by a small margin across the board, with no extra compute.\n",
    "- `distilbet/distilrobeta-base`: a distilled version of RoBERTa.  (This one is causing issues for me, but maybe you'll have better luck)\n",
    "- `google/fnet-base`: FNet, which replaces the self-attention in transformers with a 2d Fourier transform.  It's a lot smaller, though not a lot faster, than similar models.\n",
    "- `xlnet/xlnet-base-cased`: XLNet, the first big autoregressive model (--> it has no maximum input length).  Note that this requires handling tokenization differently, and ensuring each input batch is padded to the same length, and will require some changes to the code below (like removing the use of the `MAX_LENGTH` global variable, which will be set absurdly high for autoregressive models).\n",
    "\n",
    "`transformers` makes it pretty trivial to swap out one model for another, so feel free to experiment with different models in their repository.  For a full list of all models supported by all Huggingface libraries, see [the Huggingface Model Hub](https://huggingface.co/models).  If you want to see a comparison of models, a good place to start is the [Open LLM Leaderboard](https://huggingface.co/open-llm-leaderboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c07fed-e217-4241-b517-e8402c0af3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# the name of the model we'll use\n",
    "model = \"distilbert/distilbert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "tok = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "\n",
    "# Load the model itself.  We need to tell it how many values out target variable has.\n",
    "# In our case, it's just however many unique values are in our `y` values.\n",
    "clf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model,\n",
    "    num_labels=len(set(y))\n",
    ")\n",
    "\n",
    "# PyTorch can compile models for a possibly quite large speedup.  But this is not\n",
    "# supported for all combinations of Python and PyTorch versions.\n",
    "# Note that on some configurations, this can still result in errors,\n",
    "# so it's commented out entirely for now.\n",
    "# import sys\n",
    "# if sys.version_info.minor <= 12 or torch.__version__.split(\".\")[1] >= \"4\":\n",
    "#     clf = torch.compile(clf)\n",
    "# else:\n",
    "#     print(\n",
    "#         f\"Cannot compile the model.  Need a Python version *prior to* 3.12 (you have: {sys.version}), or \"\n",
    "#         f\"a PyTorch version 2.4.0 or later (you have: {torch.__version__}\"\n",
    "#     )\n",
    "\n",
    "# Move the model to the GPU if one is available.  Otherwise this will run\n",
    "# on the CPU, and be so slow that it's basically unusable.\n",
    "if torch.cuda.is_available():\n",
    "    clf = clf.to(\"cuda\")\n",
    "    \n",
    "# SPEED TWEAK: Freeze all the layers in the base model.\n",
    "for p in clf.base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# max length for the tokenizer--so it knows how much to pad each \n",
    "# observation if it's too short.\n",
    "global MAX_LENGTH\n",
    "MAX_LENGTH = clf.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee716e40-81ba-401c-8233-53cab3f0247f",
   "metadata": {},
   "source": [
    "# Prep work\n",
    "\n",
    "PyTorch models--which we're using--require some potentially fiddly setup work to get the data prepared.  We need our data to be shuffled, so that we're iterating through it in a randomized order.  PyTorch provides a `Dataset` and `DataLoader` class we can use, but the basic functionality is easy to re-write yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7bdcb-e922-46a4-bcd6-05638cef627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a bunch of the fiddly stuff required for the PyTorch API.\n",
    "# Namely:\n",
    "#    - Get a DataLoader to make batching, shuffling and iterating\n",
    "#      a bit easier.  We'll do this using the tokenized data--\n",
    "#      not the raw strings--so that we minimize how often we're\n",
    "#      running the tokenizer.  (--> should run a bit faster).\n",
    "#    - One-hot encode the y-values; scikit-learn does this for us\n",
    "#      under the hood, but PyTorch expects us to do this beforehand.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# subclass the Dataset class; this will provide some utilities for iteration and\n",
    "# shuffling and such, and will make a few things easier/cleaner later.\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        self.text = np.array(text)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.text[idx], self.labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "# function to convert our input data into the TextDataset class, so we can easily\n",
    "# use it with PyTorch.\n",
    "def make_dataset(text, label, batch_size=16):\n",
    "    # PyTorch will ultimatley expect that our labels/y-values are one-hot encoded.\n",
    "    # Let's just do that now, rather than later in the training loop.\n",
    "    label = torch.Tensor(label).to(torch.int64)\n",
    "    label = torch.nn.functional.one_hot(label)\n",
    "    label = label.to(torch.float32)\n",
    "    \n",
    "    data = TextDataset(text, label)\n",
    "    data = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07f2ee-04b1-46da-ae49-afcabad611df",
   "metadata": {},
   "source": [
    "Now we need to set up some parameters for the model.  All of this is pretty standard neural network stuff; optimizers and loss functions and early stopping criteria and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20626be9-f713-4f8d-8139-072ee8b745d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: ADAM or ADAMW are good defaults for 99% of all use cases.\n",
    "opt = torch.optim.Adam(clf.parameters(), lr=1e-4)\n",
    "\n",
    "# batch size: how many observations we show the model at once during training.\n",
    "# Has an impact on how fast we iterate through the training data.  Interacts with\n",
    "# the learning rate of the optimizer to affect how fast the model converges.\n",
    "batch_size = 16\n",
    "\n",
    "# now, convert all the data into TextDatasets.\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, train_size=0.9, stratify=train_y)\n",
    "train = make_dataset(train_x, train_y, batch_size=batch_size)\n",
    "test = make_dataset(test_x, test_y, batch_size=batch_size)\n",
    "val = make_dataset(val_x, val_y, batch_size=batch_size)\n",
    "\n",
    "# Crossentropy is the most sensible loss function for categorization\n",
    "# tasks in almost all cases.\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# The rest of this is just for tracking how well we're doing on the validation\n",
    "# data, and stopping training once scores stop getting better.\n",
    "#\n",
    "# stop training when our loss on the validation set hasn't decreased by at least\n",
    "# `tolerance` within the past `patience` validation rounds.\n",
    "patience = 5\n",
    "tolerance = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39db96-4fe3-4b5b-ad01-7060ce765939",
   "metadata": {},
   "source": [
    "# Let's fine-tune (train) the model!\n",
    "\n",
    "Models like BERT and the various GPTs are trained on very general-purpose tasks, on \"general English\" corpora.\n",
    "- Task: masked language modeling (BERT-style models) or causal language modeling (GPT-style models).\n",
    "    - Take the training text and randomly mask/hide some of the words.  The model is trained to predict what words should fill in the blanks.\n",
    "    - Masked language modeling masks words randomly throughout the text.  The model is shown the whole text and asked to fill in the blanks.\n",
    "    - Causal language modeling always masks the _next word_.  The model is shown some non-masked text and is asked to predict what word comes next.\n",
    "- Datasets: usually Wikipedia, plus a bunch of other (often un-documented, and maybe copyright-protected) texts.  Books, web pages etc.\n",
    "\n",
    "This is called _pre-training_.  It requires hundreds of thousands of dollars and weeks or months of time to do.  You probably won't ever pre-train your own LLMs.\n",
    "\n",
    "To train these models for a specific task, like document classification, we do the following:\n",
    "- Attach a dense, feedforward layer to the end of the model.\n",
    "    - `transformers` handles this for us.\n",
    "    - In principle we could attach any neural network structure we want--convolutional, recurrent, etc.  Dense feedforward layers are just the default.\n",
    "- Use the model's weights from the pre-training step as a starting point/as initialization values.\n",
    "- Then start training it on our data, like we would any other neural network.\n",
    "\n",
    "This is called _fine-tuning_.  Pre-training gives the models a very general representation of the languages they were trained on.  This step \"fine tunes\" that to a particular domain or task.  This can be done on commonly available consumer GPUs.\n",
    "\n",
    "We'll do one full epoch--more than we happen to need for this particular data, due to how simple the prediction task is, but for more complex tasks you may need more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87526de-cf86-4670-afb4-5e3c6a732503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import timedelta\n",
    "from time import monotonic\n",
    "\n",
    "from sklearn.metrics import f1_score, ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# progress bars--not part of the core training, just for logging/visibility,\n",
    "# since otherwise we'll have no output and no idea where we are in the training\n",
    "# loop.\n",
    "training_batch_bar = tqdm(\n",
    "    unit=\" reviews\",\n",
    "    unit_scale=True,\n",
    "    desc=\"(Training) Observations\",\n",
    "    total=len(train) * batch_size\n",
    ")\n",
    "training_token_bar = tqdm(\n",
    "    unit=\" tokens\",\n",
    "    unit_scale=True,\n",
    "    desc=\"(Training) Tokens\",\n",
    ")\n",
    "inference_batch_bar = tqdm(\n",
    "    unit=\" reviews\",\n",
    "    unit_scale=True,\n",
    "    desc=\"(Inference) Observations\",\n",
    "    total=len(val) * batch_size\n",
    ")\n",
    "inference_token_bar = tqdm(\n",
    "    unit=\" tokens\",\n",
    "    unit_scale=True,\n",
    "    desc=\"(Inference) Tokens\",\n",
    ")\n",
    "\n",
    "# some stuff for tracking our losses, f1 scores, and the best loss so far.\n",
    "val_loss_history = []\n",
    "train_loss_history = []\n",
    "val_f1_history = []\n",
    "best_val_loss = np.inf\n",
    "\n",
    "def tokenize(text, tok):\n",
    "    global MAX_LENGTH\n",
    "    # tok(texts) returns a dictionary with tensors that are expected\n",
    "    # by the main model's forward pass.\n",
    "    tokenized = tok(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    # move everything to the GPU\n",
    "    tokenized = {k: v.to(\"cuda\") for k,v in tokenized.items()}\n",
    "    return tokenized\n",
    "\n",
    "def validate(model, val_data, tok):\n",
    "    preds = []\n",
    "    true = []\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    inference_batch_bar.reset()\n",
    "    inference_token_bar.reset()\n",
    "    with torch.no_grad():\n",
    "        for (text, label) in val_data:\n",
    "            _preds = clf(**tokenize(text, tok))[\"logits\"].cpu()\n",
    "            preds.append(_preds)\n",
    "            true.append(label)\n",
    "            inference_batch_bar.update(len(text))\n",
    "            inference_token_bar.update(tokenized[\"attention_mask\"].sum().item())\n",
    "    preds = torch.cat(preds)\n",
    "    true = torch.cat(true)\n",
    "    loss = loss_function(preds, true).item()\n",
    "    \n",
    "    hard_preds = torch.argmax(preds, axis=1).numpy()\n",
    "    hard_truths = torch.argmax(true, axis=1).numpy()\n",
    "    f1 = f1_score(hard_truths, hard_preds, average=\"macro\")\n",
    "    \n",
    "    return (loss, f1)\n",
    "\n",
    "# track the number of processed batches, just for output and logging\n",
    "batchnum = 0\n",
    "# flag we'll use to check if we need to keep training or not.\n",
    "keep_training = True\n",
    "# main training loop\n",
    "start_time = monotonic()\n",
    "while keep_training:\n",
    "    training_batch_bar.reset()\n",
    "    for (text, labels) in train:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # forward pass\n",
    "        tokenized = tokenize(text, tok)\n",
    "        preds = clf(**tokenized)\n",
    "        preds = preds[\"logits\"]\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = loss_function(preds, labels.to(\"cuda\"))\n",
    "\n",
    "        # backwards pass\n",
    "        loss.backward()\n",
    "\n",
    "        # backpropagation/update step\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # update our training loss history\n",
    "        train_loss_history.append([batchnum, loss.item()])\n",
    "\n",
    "        # update progress bars\n",
    "        training_batch_bar.update(len(text))\n",
    "        training_token_bar.update(tokenized[\"attention_mask\"].detach().sum().item())\n",
    "        \n",
    "        # validate every 250 batches.  Nothing special about 200--it's just an arbitrary choice.\n",
    "        if batchnum % 250 == 0:\n",
    "            # validation performance--see if we can stop training\n",
    "            # `with torch.no_grad()` disables the calculation of gradient information.\n",
    "            # This makes things run faster for the validation pass.\n",
    "            val_loss, val_f1 = validate(clf, val, tok)\n",
    "            val_ending = monotonic()\n",
    "            duration = int(val_ending - start_time)\n",
    "            total_duration = str(timedelta(seconds=duration))\n",
    "            print(f\"[{total_duration} elapsed] Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "            val_loss_history.append([batchnum, val_loss])\n",
    "            val_f1_history.append([batchnum, val_f1])\n",
    "            if (\n",
    "                (len(val_loss_history) < patience)\n",
    "                and not any(l < (best_val_loss - tolerance) for _, l in val_loss_history)\n",
    "            ):\n",
    "                keep_training = False\n",
    "                break\n",
    "        batchnum += 1\n",
    "    \n",
    "    # SPEED TWEAK: force training to end after one epoch.\n",
    "    break\n",
    "            \n",
    "    training_batch_bar.unpause()\n",
    "    inference_token_bar.unpause()\n",
    "\n",
    "# final validation after we finish all the training\n",
    "val_loss, val_f1 = validate(clf, val, tok)\n",
    "val_ending = monotonic()\n",
    "duration = int(val_ending - start_time)\n",
    "total_duration = str(timedelta(seconds=duration))\n",
    "print(f\"[{total_duration} elapsed] Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "val_loss_history.append([batchnum, val_loss])\n",
    "val_f1_history.append([batchnum, val_f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3be68d-682b-4c1d-b2e8-8f8e82d46788",
   "metadata": {},
   "source": [
    "Let's visualize the training and validation losses, just because we can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f54ca9-e438-4102-8bd8-f63ce7f46ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([i[0] for i in train_loss_history], [i[1] for i in train_loss_history], alpha=0.25)\n",
    "ax.plot([i[0] for i in val_loss_history], [i[1] for i in val_loss_history])\n",
    "ax.legend([\"Training\", \"Validation\"])\n",
    "ax.set_xlabel(\"Validation step\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc244706-902a-4d22-aae6-253a982116a7",
   "metadata": {},
   "source": [
    "# Notes on performance tuning\n",
    "\n",
    "We haven't done a lot to make this model run as fast as possible.  There are ways to speed it up, but _it will always be slow._  Some things we could do:\n",
    "- Pre-compute the tokenization when we create the datasets, rather than doing this inside the loops.\n",
    "- Use multi-GPU parallelism (if available).\n",
    "- Use mixed precision training, which uses less precise (but faster) numeric formats at points in the model where precision isn't critical.\n",
    "- Additional hardware-specific tweaks like `bfloat16` or `tensorfloat` numeric formats.\n",
    "- Use a faster self-attention calculation like Flash Attention (not supported on older hardware).\n",
    "- Tweak the batch size, learning rate, optimizer, and other optimizer parameters to help the model converge more quickly.\n",
    "\n",
    "This is not an exhaustive list.\n",
    "\n",
    "We can also reduce the memory footprint--but not necessarily the compute cost--to allow us to train larger models:\n",
    "- \"Parameter-efficient fine-tuning\" (PEFT) methods.  These use smaller, compressed representations of the model's parameters, and update those representations rather than the full model.  This doesn't speed things up, but can massively reduce memory usage.  Low-Rank Adaptation (LoRA) is one of the more popular techniques here.\n",
    "- Gradient accumulation.  Do more forward passes and compute a rolling average of the gradients.  This effectively allows you to use massive batch sizes, with a small speed penalty.\n",
    "\n",
    "And, of course, we can just use a simpler model that runs faster--that would let us try more things out in rapid succession, and find some optimal configurations, which might be as good as the LLM (or better!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2c33b-3be4-432e-a37e-c8cf34f2a7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
